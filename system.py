import os
import re
import numpy as np
import torch
from sentence_transformers import SentenceTransformer, util
from elasticsearch import Elasticsearch, helpers

# Khá»Ÿi táº¡o model
model = SentenceTransformer("minilm-custom-model/final")

# Káº¿t ná»‘i Elasticsearch
ES_HOST = "http://localhost:9200"
INDEX_NAME = "documents"
es = Elasticsearch([ES_HOST])

# ThÆ° má»¥c chá»©a cÃ¡c file text
TEXT_FOLDER = "search_results/downloaded_texts"

# HÃ m Ä‘á»c ná»™i dung tá»« file .txt
def read_text_file(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘á»c file {file_path}: {e}")
        return ""

# HÃ m chia ná»™i dung thÃ nh tá»«ng Ä‘oáº¡n
def split_text_into_paragraphs(text):
    paragraphs = text.split('\n')
    paragraphs = [p.strip() for p in paragraphs if p.strip()]
    return paragraphs

# HÃ m táº¡o index trÃªn Elasticsearch
def create_index():
    if es.indices.exists(index=INDEX_NAME):
        es.indices.delete(index=INDEX_NAME)

    es.indices.create(
        index=INDEX_NAME,
        body={
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "filename": {"type": "keyword"},
                    "paragraph": {"type": "text"},
                    "paragraph_hash": {"type": "keyword"},  # ğŸ”¥ ThÃªm trÆ°á»ng hash Ä‘á»ƒ há»— trá»£ khá»­ trÃ¹ng
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 384,
                        "index": True,
                        "similarity": "cosine"
                    }
                }
            }
        }
    )
    print("âœ… ÄÃ£ táº¡o index Elasticsearch")

# HÃ m táº¡o hash tá»« ná»™i dung Ä‘oáº¡n vÄƒn Ä‘á»ƒ khá»­ trÃ¹ng
def create_paragraph_hash(paragraph):
    # Loáº¡i bá» khoáº£ng tráº¯ng vÃ  chuyá»ƒn vá» chá»¯ thÆ°á»ng Ä‘á»ƒ so sÃ¡nh chÃ­nh xÃ¡c ná»™i dung
    normalized_para = re.sub(r'\s+', '', paragraph.lower())
    return hash(normalized_para)

# HÃ m lÆ°u dá»¯ liá»‡u vÃ o Elasticsearch
def index_documents():
    create_index()
    actions = []
    batch_size = 500
    indexed_count = 0
    skipped_count = 0
    paragraph_hashes = set()  # ğŸ”¥ Táº­p há»£p lÆ°u cÃ¡c hash Ä‘á»ƒ kiá»ƒm tra trÃ¹ng láº·p

    for filename in os.listdir(TEXT_FOLDER):
        file_path = os.path.join(TEXT_FOLDER, filename)
        text_content = read_text_file(file_path)

        if not text_content.strip():
            print(f"âš ï¸ Bá» qua file rá»—ng: {filename}")
            continue

        paragraphs = split_text_into_paragraphs(text_content)

        for para in paragraphs:
            # ğŸ”¥ Kiá»ƒm tra vÃ  bá» qua Ä‘oáº¡n vÄƒn quÃ¡ ngáº¯n
            if len(para.split()) < 5:  # Bá» qua Ä‘oáº¡n vÄƒn cÃ³ Ã­t hÆ¡n 5 tá»«
                skipped_count += 1
                continue
                
            para_hash = create_paragraph_hash(para)
            
            # ğŸ”¥ Kiá»ƒm tra trÃ¹ng láº·p
            if para_hash in paragraph_hashes:
                skipped_count += 1
                continue
                
            paragraph_hashes.add(para_hash)
            
            embedding = model.encode(para).tolist()
            actions.append({
                "_index": INDEX_NAME,
                "_source": {
                    "filename": filename,
                    "paragraph": para,
                    "paragraph_hash": str(para_hash),  # LÆ°u hash Ä‘á»ƒ dá»… truy váº¥n sau nÃ y
                    "embedding": embedding
                }
            })

            indexed_count += 1

            if len(actions) >= batch_size:
                helpers.bulk(es, actions)
                actions = []

    if actions:
        helpers.bulk(es, actions)

    print(f"âœ… ÄÃ£ index {indexed_count} Ä‘oáº¡n vÄƒn vÃ o Elasticsearch")
    print(f"âš ï¸ ÄÃ£ bá» qua {skipped_count} Ä‘oáº¡n vÄƒn (quÃ¡ ngáº¯n hoáº·c trÃ¹ng láº·p)")
# HÃ m tÃ¡ch cÃ¢u tá»« Ä‘oáº¡n vÄƒn
def split_into_sentences(text):
    text = re.sub(r'\.+', '.', text)  # XoÃ¡ dáº¥u cháº¥m dÆ° thá»«a (.. -> .)
    sentences = re.split(r'\.\s+', text)  # TÃ¡ch cÃ¢u dá»±a trÃªn dáº¥u cháº¥m vÃ  khoáº£ng tráº¯ng
    sentences = [s.strip() for s in sentences if s.strip()]  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
    return sentences

# HÃ m tÃ¬m kiáº¿m dá»±a trÃªn tá»«ng cÃ¢u cá»§a input
def hybrid_search_by_sentence(input_text, top_k=10, num_candidates=200):
    input_sentences = split_into_sentences(input_text)  # TÃ¡ch input thÃ nh cÃ¡c cÃ¢u nhá»
    
    all_results = []
    
    for sentence in input_sentences:
        print(f"\nğŸ” TÃ¬m kiáº¿m cho cÃ¢u: \"{sentence}\"")
        input_embedding = model.encode(sentence).tolist()

        query = {
            "size": top_k * 2,
            "knn": {
                "field": "embedding",
                "query_vector": input_embedding,
                "k": top_k * 2,
                "num_candidates": num_candidates
            }
        }

        response = es.search(index=INDEX_NAME, body=query)
        results = response["hits"]["hits"]
        
        unique_results = {}

        for res in results:
            filename = res["_source"]["filename"]
            paragraph = res["_source"]["paragraph"]
            para_hash = res["_source"]["paragraph_hash"]
            para_embedding = res["_source"]["embedding"]
            
            score = util.cos_sim(
                torch.tensor(input_embedding, dtype=torch.float32),
                torch.tensor(para_embedding, dtype=torch.float32)
            ).item()

            if para_hash not in unique_results:
                unique_results[para_hash] = {
                    "filename": filename,
                    "paragraph": paragraph,
                    "score": score
                }

        sorted_results = sorted(unique_results.values(), key=lambda x: x["score"], reverse=True)[:top_k]
        all_results.extend(sorted_results)
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ cho tá»«ng cÃ¢u input
        for i, result in enumerate(sorted_results, 1):
            print(f"#{i} ğŸ“„ File: {result['filename']}")
            print(f"ğŸ”¹ Äoáº¡n vÄƒn: {result['paragraph']}")
            print(f"ğŸ“Š Äá»™ tÆ°Æ¡ng Ä‘á»“ng: {result['score']:.4f}\n")

    # ğŸ”¥ TÃ­nh Ä‘á»™ Ä‘a dáº¡ng cá»§a káº¿t quáº£ tá»•ng há»£p
    unique_files = len(set(result["filename"] for result in all_results))
    print(f"\nğŸ“‘ Tá»•ng sá»‘ lÆ°á»£ng file duy nháº¥t á»Ÿ trong táº¥t cáº£ káº¿t quáº£: {unique_files}/{len(all_results)}")
